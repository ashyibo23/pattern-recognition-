{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ashPNNEXAM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJFobfOVrONw"
      },
      "source": [
        "**PesudoInverse** **Ax = b**\n",
        "\n",
        "finding weight using least square method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5ze54gQq_nm"
      },
      "source": [
        "#inverse Ax = b\n",
        "\n",
        "import numpy as np\n",
        "a = np.array([[10, 8, -3, 1],\n",
        "              [8, 10, -3, 1],\n",
        "              [3, 3, -1, 1],\n",
        "              [1, 1, -1, 0]])\n",
        "b = np.array([1, 1, -1, 0])\n",
        "t = np.transpose(a)\n",
        "c = t.dot(a)\n",
        "inverse = np.linalg.inv(c) #inverse\n",
        "\n",
        "#next part \n",
        "next_part = t.dot(b)\n",
        "final_part = inverse.dot(next_part)\n",
        "print(final_part)\n",
        "\n",
        "\n",
        "#if you use this code for svm the last element of the output is w0 (basis weight) so we ignore it to find weight\n",
        "#but we use to find for margin if go with hyperplane w^Tx+w0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-uodbgE1FvC"
      },
      "source": [
        "**Week 1 Introduction**\n",
        "\n",
        "Draw a confusion matrix for this data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "lZ6We6t91RLd",
        "outputId": "d924ab93-faa3-457f-a5b5-8e62bfa04cdc"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, f1_score\n",
        "\n",
        "\n",
        "def basic_metrics(y_true, y_pred, class_names, avg, normalize=False):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm = cm[:,::-1][::-1]\n",
        "    np.set_printoptions(precision=4)\n",
        "    \n",
        "    title='Confusion matrix'\n",
        "    cmap=plt.cm.Blues\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    \n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "    print(classification_report(y_true, y_pred, target_names=class_names[::-1],digits=4))\n",
        "    print()\n",
        "    print(\"Overall\", \"\\t\", \"Precision\", \"\\t\", \"Recall\",\"\\t\", \"F1 Score\")\n",
        "    print(\"\\t\\t\",\"{:.4f}\".format(precision_score(y_true,y_pred,average=avg)),\"\\t\", \n",
        "          \"{:.4f}\".format(recall_score(y_true,y_pred,average=avg)), \"\\t\",  \"{:.4f}\".format(f1_score(y_true,y_pred,average=avg)))\n",
        "    print(\"Close figure to terminate.\")\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Both Binary and multi classification works here (Choose average = binary for binary classification)\n",
        "    class_names = np.array([\"Two\",\"One\", \"Zero\"]) #Name of classes in descending order\n",
        "    y_true = [1,1,0,1,0,1,1,2] #True value of class labels \n",
        "    y_pred = [1,0,1,1,0,1,0,2] #Predicted value of class labels\n",
        "    average = 'micro' # Choose average parameter for overall metrics (binary, micro, macro, weighted)\n",
        "    basic_metrics(y_true, y_pred, class_names,average)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "#output is one row \n",
        "#for matrix it's 2by2 from one row which is from last two lost column"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Zero     0.3333    0.5000    0.4000         2\n",
            "         One     0.7500    0.6000    0.6667         5\n",
            "         Two     1.0000    1.0000    1.0000         1\n",
            "\n",
            "    accuracy                         0.6250         8\n",
            "   macro avg     0.6944    0.7000    0.6889         8\n",
            "weighted avg     0.6771    0.6250    0.6417         8\n",
            "\n",
            "\n",
            "Overall \t Precision \t Recall \t F1 Score\n",
            "\t\t 0.6250 \t 0.6250 \t 0.6250\n",
            "Close figure to terminate.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j34bhn2xa5hQ"
      },
      "source": [
        "## Widrow-Hoff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B4InZkkbM08"
      },
      "source": [
        "def widrow_hoff_accuracy(X_t, Y, at):\n",
        "    correct = 0\n",
        "    for i in range(len(Y)):\n",
        "        true_label = Y[i, 0]\n",
        "\n",
        "        y = np.concatenate((np.array([1]), X_t[i]))\n",
        "        y = y.reshape(len(y), 1)\n",
        "\n",
        "        # calculate ay\n",
        "        ay = np.dot(at, y)\n",
        "\n",
        "        if ay > 0: predicted_label = 1\n",
        "        else: predicted_label = -1\n",
        "\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "\n",
        "    return correct/len(Y)\n",
        "\n",
        "def seq_widrow_hoff(X, Y, at, bt, lr, iterations):\n",
        "    result = []\n",
        "    for o in range(int(iterations / len(Y))):\n",
        "        for i in range(len(Y)):\n",
        "            at_prev = at\n",
        "            y = np.concatenate((np.array([1]), X[i]))\n",
        "            if Y[i] < 0:\n",
        "                y = y * -1\n",
        "            y = y.reshape(len(y), 1)\n",
        "\n",
        "            # calculate ay\n",
        "            ay = np.dot(at, y).round(4)\n",
        "\n",
        "            # calculate update part\n",
        "            update = (bt[0, i] - ay).round(4)\n",
        "            update = (lr * update).round(4)\n",
        "            update = (update * y.transpose()).round(4)\n",
        "\n",
        "            # add update part to a\n",
        "            at = np.add(at, update).round(4)\n",
        "\n",
        "            # append result\n",
        "            result.append((str(i + 1 + (len(Y) * o)), np.round(at_prev, 4), bt[0, i],\n",
        "                           np.round(y.transpose(), 4), np.round(ay, 4), np.round(at, 4)))\n",
        "            \n",
        "    return result, ay, at"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffEvfsf1a621"
      },
      "source": [
        "#Widrow-Hoff\n",
        "X_t = np.array([[0.0, 2.0],\n",
        "                [1.0, 2.0],\n",
        "                [2.0, 1.0],\n",
        "                [-3.0, 1.0],\n",
        "                [-2.0, -1.0],\n",
        "                [-3.0, -2.0]])\n",
        "\n",
        "Y = np.array([[1, 1, 1, -1, -1, -1]]).transpose()  #output class\n",
        "\n",
        "\n",
        "at = np.array([[1.0, 0.0, 0.0]])  # initial param weights\n",
        "bt = np.array([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]])  # margin vector\n",
        "lr = 0.1  # learning rate\n",
        "epochs = 2\n",
        "iterations = epochs * len(X_t)\n",
        "\n",
        "result, ay, at_new = seq_widrow_hoff(X_t, Y, at, bt, lr, iterations)\n",
        "\n",
        "\n",
        "# Print results\n",
        "# -----------------------------------------------------------\n",
        "pt = PrettyTable(('iteration', 'at', 'b', 'y_trans', 'at.y', 'at_new'))\n",
        "for row in result: pt.add_row(row)\n",
        "\n",
        "pt.align['iteration'] = 'c'\n",
        "pt.align['at'] = 'l'\n",
        "pt.align['b'] = 'l'\n",
        "pt.align['y_trans'] = 'l'\n",
        "pt.align['at.y'] = 'l'\n",
        "pt.align['at_new'] = 'l'\n",
        "\n",
        "print(pt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQM_8ij1BBVQ"
      },
      "source": [
        "**WEEK 2**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw6TRzEmOj0M"
      },
      "source": [
        "Question 5\n",
        "\n",
        "In augmented feature space, a dichotomizer is defined using the following linear discriminant function g(x) = a^ty  and y is y^t = (1, x) where x is the input vector and 1 is augmented.\n",
        "\n",
        "Normally threshold is zero anything greater than zero class 1 otherwise class 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z-uQZzAOZ42",
        "outputId": "61276e8f-3151-4c22-b74c-1795a7b0efe4"
      },
      "source": [
        "import numpy as np\n",
        "#weights with bias and w1 w2...\n",
        "at=np.array([-3, 1, 2, 2, 2, 4])  \n",
        "#input vector x\n",
        "x1=np.array([1, 0, -1, 0, 0, 1]).T  #1 infront in the vector is augmented \n",
        "#input vector \n",
        "x2=np.array([1, 1, 1, 1, 1, 1]).T #1 infront in the vector is augmented\n",
        "\n",
        "print(np.dot(at,x1),  '\\n')\n",
        "print(np.dot(at,x2), '\\n')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1 \n",
            "\n",
            "8 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnMdw2uiTUWD"
      },
      "source": [
        "**Question 6:** \n",
        "<br>\n",
        "A Linear Discriminant Function is used to define a Dichotomizer, such that x is assigned to class 1 if g(x) > 0,\n",
        "and x is assigned to class 2 otherwise. Use the Batch Perceptron Learning Algorithm (with augmented notation\n",
        "and sample normalisation), to find appropriate parameters for the linear discriminant function, when the data set\n",
        "is as shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Jcf6G6TVF-"
      },
      "source": [
        "#no change in weight sign. same as in the question but sample normalisation with sign\n",
        "#if output class is negative [4, 1] ---> [-1, -4, -1]\n",
        "\n",
        "import numpy as np \n",
        "from prettytable import PrettyTable\n",
        "from sklearn import datasets\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Given Parameters in Question:\n",
        "a = [-25, 6, 3]         # Initial a\n",
        "n = 1                   # Learning Rate\n",
        "\n",
        "# Given Dataset:\n",
        "X = [[1, 5], [2, 5], [4, 1], [5, 1]]\n",
        "Y = [1, 1, 2, 2]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Applying Sample Normalisation:\n",
        "Norm_Y = []\n",
        "\n",
        "for x, y in zip(X, Y):\n",
        "    \n",
        "    # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
        "    if y == 2 or y == -1:\n",
        "        x = [i * -1 for i in x]\n",
        "        x.insert(0, -1)\n",
        "        Norm_Y.append(x)\n",
        "    else:\n",
        "        x.insert(0, 1)\n",
        "        Norm_Y.append(x)\n",
        "\n",
        "print(\"Vectors used in Batch Perceptron Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Batch Perceptron Learning Algorithm:\n",
        "\n",
        "epoch = 1\n",
        "result=[]\n",
        "while True:\n",
        "\n",
        "    updating_samples = []\n",
        "    print(\"Epoch {}\".format(epoch))\n",
        "\n",
        "    for count, i in enumerate(range(len(Norm_Y))):\n",
        "    \n",
        "        # Knowing which value of a to use. If it is the first iteration, than use the given parameters in the \n",
        "        # question:\n",
        "        a_prev = a\n",
        "        #print(\"The value of a used is {}\".format(a_prev))\n",
        "        y_input = Norm_Y[i]\n",
        "        #print(\"y Value used for this iteration is: {}\".format(y_input))\n",
        "\n",
        "        # Equation -> g(x) = a^{t}y\n",
        "        ay = np.dot(a, y_input)\n",
        "        #print(\"The value of a^t*y for this iteration is: {}\".format(ay))\n",
        "        \n",
        "\n",
        "        result.append((a_prev, ay, ay <= 0))\n",
        "\n",
        "        # Checking if the sample is misclassified or not:\n",
        "        \n",
        "        # If sample is misclassified:\n",
        "        if ay <= 0:\n",
        "\n",
        "            # If this is the first sample in the epoch, add the previous value of a to the list of samples used \n",
        "            # for the update to perform summation at the end of the epoch:\n",
        "            if count == 0:\n",
        "                #print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
        "                updating_samples.append(np.array(a))\n",
        "                updating_samples.append(np.array(y_input))\n",
        "            \n",
        "            # If sample is misclassified and IS NOT the first sample in the epoch:\n",
        "            else:\n",
        "                #print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
        "                updating_samples.append(np.array(y_input))\n",
        "        \n",
        "        # If sample is classified correctly:\n",
        "        else: \n",
        "\n",
        "            # If first sample in the epoch, append the previous value of a to the updating samples list:\n",
        "            if count == 0:\n",
        "                updating_samples.append(np.array(a))\n",
        "                \n",
        "\n",
        "    # Calculating new value of a after having gone through all of the samples in the dataset since it is Batch Learning.\n",
        "    a_update_val = n * sum(updating_samples)\n",
        "\n",
        "    # If Block to check whether learning has converged. If we have gone through all the data without needing \n",
        "    # to update the parameters, we can conclude that learning has converged.\n",
        "    if len(updating_samples) <= 1:\n",
        "        print(\"\\nLearning has converged.\")\n",
        "        print(\"Required parameters of a are (intial weights): {}\".format(a))\n",
        "        break\n",
        "\n",
        "    # Updating a using our new value of a:\n",
        "    a = a_update_val\n",
        "    print(\"\\nNew Value of a^t updated weights for next iteration is: {}.\\n\".format(a))\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "\n",
        "pt = PrettyTable(('x', 'g(x)=ay', 'gx=<0'))\n",
        "for row in result: pt.add_row(row)\n",
        "\n",
        "pt.align['iteration'] = 'c'\n",
        "pt.align['a'] = 'l'\n",
        "pt.align['y'] = 'l'\n",
        "pt.align['ay'] = 'l'\n",
        "pt.align['a_new'] = 'l'\n",
        "\n",
        "print(pt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbU-qbpOZbJj"
      },
      "source": [
        "**Question 7**\n",
        "\n",
        "Repeat the previous question using the Sequential Perceptron Learning Algorithm (with augmented notation\n",
        "and sample normalisation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3CZiaaGY16T"
      },
      "source": [
        "##no change in weight sign. same as in the question but sample normalisation with sign\n",
        "#if output class is negative [4, 1] ---> [-1, -4, -1]\n",
        "\n",
        "# Sequential Perceptron Learning Algorithm\n",
        "\n",
        "# INPUTS:\n",
        "    # x: data\n",
        "    # y: target\n",
        "    # a: initial weights\n",
        "    # eta: learning rate\n",
        "    # n_epochs: number of epochs of learning on the entire dataset\n",
        "    # augmentation: True if data is augmented\n",
        "    # sample_normalisation: True if sample normalisation applied\n",
        "\n",
        "def sequential_perceptron_learning(x, y, a, eta, n_epochs, augmentation, sample_normalisation):\n",
        "    n_instances = len(x)\n",
        "    \n",
        "    if augmentation:\n",
        "        x = np.insert(x, 0, 1, axis=1)\n",
        "    \n",
        "    if sample_normalisation: # CAREFUL WITH THE DATA LABEL\n",
        "        x[y==-1] = -x[y==-1]\n",
        "    \n",
        "    if sample_normalisation:\n",
        "        for n in range(n_epochs):\n",
        "            print(\"Epoch #{}: \".format(n+1))\n",
        "            for i in range(n_instances):\n",
        "                if np.sign(np.dot(a, x[i])) <= 0: # only misclassified samples\n",
        "                    a += eta*x[i]\n",
        "                print(\"Sample #{}: {}\".format(i+1, a))\n",
        "            print(\"End of epoch #{}\\n\".format(n+1))\n",
        "\n",
        "\n",
        "    else:\n",
        "        for n in range(n_epochs):\n",
        "            print(\"Epoch #{}: \".format(n+1))\n",
        "            for i in range(n_instances):\n",
        "                if np.sign(np.dot(a, x[i])) != y[i]: # only misclassified samples\n",
        "                    a += eta*y[i]*x[i]\n",
        "                print(\"Sample #{}: {}\".format(i+1, a))\n",
        "            print(\"End of epoch #{}\\n\".format(n+1))\n",
        "\n",
        "    return a\n",
        "    \n",
        "    #Using Augmented notation and sample normalisation, dataset is: which means\n",
        "    #we add 1 input vector x and adding minus to all elements in vectors depending on y output class\n",
        "    #depending on the output class postitive/negative or class 1 or 2\n",
        "\n",
        "# Tutorial example (exercise 7)\n",
        "x = np.array([[1, 5], [2, 5], [4, 1], [5, 1]]) # data\n",
        "y = np.array([1, 1, -1, -1]) # targets\n",
        "a = np.array([-25, 6, 3]) # initial weights\n",
        "\n",
        "# And here we estimate the new vector\n",
        "a = sequential_perceptron_learning(x, y, a, 1, 3, True, True)\n",
        "print(a) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5yQG5pehdtA"
      },
      "source": [
        "##Sequential Perceptron Learning Algorithm\n",
        "**Question 10** with sample normalisation\n",
        "using the sample normalisation method of implementing the Sequential Perceptron Learning Algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKVd6PaiheSN"
      },
      "source": [
        "#change in weight sign.\n",
        "#if output class is negative -1 or class 2 with input [-3, 1] ---> [-1, -3, -1]\n",
        "#[-3, -2] ---> [-1, 3, 2]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Given Parameters in Question:\n",
        "\n",
        "a = [1, 0, 0]         # Initial a where a is the weight row vector and front one is bias weight \n",
        "n = 1                 # Learning Rate\n",
        "\n",
        "# X input vector \n",
        "\n",
        "X = [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]]  \n",
        "Y = [1, 1, 1, -1, -1, -1]    #output class\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Applying Sample Normalisation:\n",
        "Norm_Y = []\n",
        "\n",
        "for x, y in zip(X, Y):\n",
        "    \n",
        "    # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
        "    if y == -1 or y == 2:\n",
        "        x = [i * -1 for i in x]\n",
        "        x.insert(0, -1)\n",
        "        Norm_Y.append(x)\n",
        "    else:\n",
        "        x.insert(0, 1)\n",
        "        Norm_Y.append(x)\n",
        "\n",
        "print(\"Vectors used in Sequential Perceptron Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Sequential Perceptron Learning Algorithm:\n",
        "\n",
        "epoch = 1\n",
        "result=[]\n",
        "while True:\n",
        "\n",
        "    updating_samples = []\n",
        "    print(\"Epoch {}\".format(epoch))\n",
        "\n",
        "    # Keeping track of how many samples are correctly classified. If this variable reaches \n",
        "    # the value that is equal to the size of the dataset (len), than we know that learning \n",
        "    # has converged:\n",
        "    correctly_classified_counter = 0\n",
        "\n",
        "    # Going through all of the samples in the dataset one-by-one:\n",
        "    for i in range(len(Norm_Y)):\n",
        "    \n",
        "        # This chooses which weight to use for an iteration. If first iteration, uses given starting weight \n",
        "        # as described in question:\n",
        "        a_prev = a\n",
        "        \n",
        "        # Selecting sample to use:\n",
        "        y_input = Norm_Y[i]\n",
        "\n",
        "        # Equation -> g(x) = a^{t}y\n",
        "        ay = np.dot(a, y_input)\n",
        "        \n",
        "\n",
        "        # Checking if the sample is misclassified or not:\n",
        "        \n",
        "        # If sample is misclassified:\n",
        "        if ay <= 0:\n",
        "\n",
        "            #print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
        "            updating_samples.append(np.array(a))\n",
        "            updating_samples.append(np.array(y_input))\n",
        "\n",
        "            # Calculating new value of a using update rule for Sequential Perceptron Learning Algorithm:\n",
        "            a_update_val = n * sum(updating_samples)\n",
        "\n",
        "\n",
        "\n",
        "            a = a_update_val\n",
        "            print(\"\\nNew Value of a^t is: {}.\\n\".format(a))\n",
        "        \n",
        "        # If the sample is correctly classified:\n",
        "        else: \n",
        "            #print(\"This sample is classified correctly.\\n\")\n",
        "            correctly_classified_counter += 1\n",
        "            pass\n",
        "            \n",
        "        # Reset sample to add for update to occur:\n",
        "        updating_samples = []\n",
        "\n",
        "        result.append((a_prev, ay, a))\n",
        "\n",
        "    # If Block to check whether learning has converged. If we have gone through all the data without needing \n",
        "    # to update the parameters, we can conclude that learning has converged.\n",
        "    if correctly_classified_counter == len(Norm_Y):\n",
        "        #print(\"\\nLearning has converged.\")\n",
        "        print(\"Required parameters of a are: {}.\".format(a))\n",
        "        break\n",
        " \n",
        "    epoch += 1\n",
        "\n",
        "\n",
        "pt = PrettyTable(('x', 'g(x)=ay', 'a_new'))\n",
        "for row in result: pt.add_row(row)\n",
        "\n",
        "pt.align['iteration'] = 'c'\n",
        "pt.align['a'] = 'l'\n",
        "pt.align['y'] = 'l'\n",
        "pt.align['ay'] = 'l'\n",
        "pt.align['a_new'] = 'l'\n",
        "\n",
        "print(pt)\n",
        "\n",
        "#first line of the output above epoch 1 is the y^t column which is the augmented and X^t normalised"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jfwKBWvv88q"
      },
      "source": [
        "##pesudoinverse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkiP_buVwAF9",
        "outputId": "18dee13c-cce2-4a26-bfd7-08c7903e86ec"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Given parameters:\n",
        "#b = np.array([2,2,2,1,1,1]\n",
        "#margin vector\n",
        "b = np.array([1]*6) # margin\n",
        "\n",
        "# Given Dataset:\n",
        "\n",
        "# Add dataset as given in question. This is assuming that Sample Normalisation has NOT been applied:\n",
        "X = [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]]\n",
        "Y = [1, 1, 1, -1, -1, -1]\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Applying Sample Normalisation:\n",
        "Norm_Y = []\n",
        "\n",
        "for x, y in zip(X, Y):\n",
        "    \n",
        "    # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
        "    if y == -1 or y == 2:\n",
        "        x = [i * -1 for i in x]\n",
        "        x.insert(0, y)\n",
        "        Norm_Y.append(x)\n",
        "    else:\n",
        "        x.insert(0, y)\n",
        "        Norm_Y.append(x)\n",
        "\n",
        "print(\"Vectors used in Pseudoinverse operation to calculate parameters of linear discriminant function:\\n {}\\n\".format(Norm_Y))\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Initialising Y Matrix:\n",
        "Y_matrix = []\n",
        "\n",
        "# Adding each normalised sample in dataset to Y Matrix:\n",
        "for i in range(len(Norm_Y)):\n",
        "    Y_matrix.append(Norm_Y[i])\n",
        "Y_matrix = np.array(Y_matrix)\n",
        "print(\"y Matrix being used:\\n {}\\n\".format(Y_matrix))\n",
        "\n",
        "# Calculating pseudo-inverse of Y Matrix:\n",
        "pseudo_inv_matrix = np.linalg.pinv(Y_matrix)\n",
        "print(\"Pseudo-inverse Matrix is:\\n {}\\n\".format(pseudo_inv_matrix))\n",
        "\n",
        "# Multiplying Pseudo-inverse matrix by given margin vector in question:\n",
        "a = np.dot(pseudo_inv_matrix, b)\n",
        "print(\"a is equal to:\\n {}\\n\".format(a))\n",
        "\n",
        "correct_classification = 0\n",
        "\n",
        "# Checking if classifications are correct:\n",
        "\n",
        "for sample in Norm_Y:\n",
        "    ay = np.dot(sample, a)\n",
        "    print(\"\\ng(x) for sample {} is {}\".format(sample, ay))\n",
        "\n",
        "    # Sample is correctly classified if ay is positive:    \n",
        "    if ay > 0:\n",
        "        print(\"Sample has been correctly classified.\")\n",
        "        correct_classification += 1\n",
        "\n",
        "if correct_classification == len(Norm_Y):\n",
        "    print(\"\\nAll samples are classified correctly which means that discriminant function parameters are correct.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSome samples are misclassified.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------------"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors used in Pseudoinverse operation to calculate parameters of linear discriminant function:\n",
            " [[1, 0, 2], [1, 1, 2], [1, 2, 1], [-1, 3, -1], [-1, 2, 1], [-1, 3, 2]]\n",
            "\n",
            "y Matrix being used:\n",
            " [[ 1  0  2]\n",
            " [ 1  1  2]\n",
            " [ 1  2  1]\n",
            " [-1  3 -1]\n",
            " [-1  2  1]\n",
            " [-1  3  2]]\n",
            "\n",
            "Pseudo-inverse Matrix is:\n",
            " [[ 0.06818182  0.16477273  0.38068182  0.10227273 -0.23295455 -0.25568182]\n",
            " [-0.03409091  0.04261364  0.18465909  0.19886364 -0.00852273  0.00284091]\n",
            " [ 0.14015152  0.07481061 -0.12026515 -0.20643939  0.11837121  0.18276515]]\n",
            "\n",
            "a is equal to:\n",
            " [0.22727273 0.38636364 0.18939394]\n",
            "\n",
            "\n",
            "g(x) for sample [1, 0, 2] is 0.6060606060606065\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [1, 1, 2] is 0.9924242424242433\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [1, 2, 1] is 1.1893939393939406\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [-1, 3, -1] is 0.7424242424242435\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [-1, 2, 1] is 0.7348484848484861\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [-1, 3, 2] is 1.3106060606060626\n",
            "Sample has been correctly classified.\n",
            "\n",
            "All samples are classified correctly which means that discriminant function parameters are correct.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSMMto4XuXrI"
      },
      "source": [
        "##Question 11\n",
        "\n",
        "##Sequential Multiclass Perceptron\n",
        "Learning algorithm to find the parameters for three linear discriminant functions that will correctly classify this\n",
        "data. Assume initial values for all parameters are zero, and use a learning rate of 1. If more than one discriminant\n",
        "function produces the maximum output, choose the function with the highest index (i.e., the one that represents\n",
        "the largest class label)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmc0G8fItQ4c"
      },
      "source": [
        "# Discriminant Functions [week 2]\n",
        "import numpy as np\n",
        "\n",
        "def sequential_multiclass_perceptron_learning (N, augmented_matrix, eta, omega, number_of_classes, number_of_features ):\n",
        "  # N is the number of exemplars provided in the question\n",
        "  # augmented_matrix is the augmented feature vector from the question\n",
        "  # eta is the learning rate given in the question\n",
        "  # omega is an array containing all the output classes of the feature vectors\n",
        "\n",
        "  N_counter = 0 # counter which keeps track of cases where winner_class == omega[index]\n",
        "\n",
        "  #Step 2. Initialise aj for each class\n",
        "  at = np.zeros((number_of_classes, number_of_features))\n",
        "\n",
        "  for i in range(0,15):\n",
        "    print ('Iteration: ',i+1)\n",
        "    # Step 3. Find values of g1, g2 and g3 and then select the arg max of g\n",
        "    index = i % 5\n",
        "\n",
        "    #Print updated a^t value\n",
        "    print('a^t:')\n",
        "    print(at)\n",
        "    \n",
        "    # Compute g value\n",
        "    g = np.empty([number_of_classes])\n",
        "    for i in range(len(g)):\n",
        "      print('Calculation of g values..........')\n",
        "      print('a^t is:',at[i])\n",
        "      print('Index is:', index)\n",
        "      print('Aug y^t:', augmented_matrix[:,index] )\n",
        "      g[i] = at[i] @ augmented_matrix[:,index]\n",
        "\n",
        "\n",
        "    print('g1 | g2 | g3')\n",
        "    print(g)\n",
        "\n",
        "    #Step 4. Select the winner\n",
        "    #Logic for 0,0,0 case and similar ones where 2 gs can produce max value\n",
        "    seen = []\n",
        "    bRepeated = False\n",
        "    # Check if there are multiple max values, and assign the winner class accordingly\n",
        "    for number in g:\n",
        "        if number in seen:\n",
        "          bRepeated = True\n",
        "          print (\"Number repeated!\")\n",
        "          m = max(g)\n",
        "          temp = [index for index, j in enumerate(g) if j == m]\n",
        "          winner_class = max(temp) + 1\n",
        "        else:\n",
        "            seen.append(number)\n",
        "    #If all g values are unique, simply select the max value's class as the winner\n",
        "    if(bRepeated == False):\n",
        "      g = g.tolist()\n",
        "      arg_max = max(g)\n",
        "      winner_class = g.index(arg_max) + 1\n",
        "    \n",
        "    print('Winner class = ', winner_class, ', and actual class is:',omega[index])\n",
        "\n",
        "    #Compare winnner to actual class \n",
        "    if(winner_class != omega[index]):\n",
        "      # Step 4. Apply the update rule as per the algorithm \n",
        "      \n",
        "      #Increment the actual class value which is incorrectly classified \n",
        "      at[omega[index]-1] = at[omega[index]-1] + eta * augmented_matrix[:,index]\n",
        "      print('New loser value:', at[omega[index]-1])\n",
        "\n",
        "      #Penalize the wrongly predicted Winner class\n",
        "      at[winner_class-1] = at[winner_class-1] - eta * augmented_matrix[:,index]\n",
        "      print('New winner value:', at[winner_class-1])\n",
        "\n",
        "      #Reset counter to 0\n",
        "      N_counter =0\n",
        "    else:\n",
        "      print ('No update is performed!')\n",
        "      N_counter +=1 #Increment convergence counter which keeps track of cases where winner_class == omega[index]\n",
        "      if(N_counter == N): ## check for convergence\n",
        "        print('Value of N = ', N)\n",
        "        print('Value of N_counter = ', N_counter)\n",
        "        print('Learning has converged, so stopping...')\n",
        "        print ('Final values of a^t after update....')\n",
        "        print('at')\n",
        "        print(at)\n",
        "        break\n",
        "      print ('N counter value = ', N_counter)\n",
        "    print('at')\n",
        "    print(at)\n",
        "    print ('=========================================================')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #Set input variables for the sequential_multiclass_perceptron_learning function\n",
        "    N = 5 # N refers to the number of exemplars in the input dataset\n",
        "    eta = 1\n",
        "    input_array = np.array([[ 1,  1,  1,  1],\n",
        "                           [ 2,  0, -1, -1],\n",
        "                           [ 0,  2,  1, -1]]) # Input matrix from the question\n",
        "    augmented_matrix = np.insert(input_array,0,1,axis=1)\n",
        "    omega = np.array([1,1,2,2,3]) # Class labels from the question \n",
        "    number_of_classes = 3\n",
        "    number_of_features = 3\n",
        "\n",
        "    #Call function\n",
        "    sequential_multiclass_perceptron_learning (N, augmented_matrix, eta, omega, number_of_classes, number_of_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpIqmowsN834"
      },
      "source": [
        "##Week 3\n",
        "\n",
        "**Question 2**\n",
        "<br>\n",
        " A neuron has a transfer function which is a linear weighted sum of its inputs and an activation function that\n",
        "is the Heaviside function. If the weights are w"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-aNFDKoN9sO"
      },
      "source": [
        "#heaviside funtion\n",
        "import numpy as np\n",
        "\n",
        "#weights with bias and w1 w2...\n",
        "weight =np.array([0.1, -0.5, 0.4])  \n",
        "#input vector x\n",
        "x1=np.array([0.1, -0.5, 0.4]).T  #1 infront in the vector is augmented \n",
        "#input vector \n",
        "x2=np.array([0.1, 0.5, 0.4]).T #1 infront in the vector is augmented\n",
        "\n",
        "print(np.dot(at,x1),  '\\n')\n",
        "print(np.dot(at,x2), '\\n')\n",
        "\n",
        "#once output generated it needs to classify it's class by the threshold set in the question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuHDCLjjVotX"
      },
      "source": [
        "**Question 3**\n",
        "<br>\n",
        "A Linear Threshold Unit. Apply the sequential Delta learning rule so that the output of this neuron\n",
        "\n",
        "here weight threshold theta (bias) sign change to negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5J_gbI9PyUi"
      },
      "source": [
        "#if intiial weights of theta = 0.5 and w1 = 2 using augmentation-----> [-0.5, 2]\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def heaviside_half(x):\n",
        "    if x < 0: return 0\n",
        "    if x == 0: return 0.5\n",
        "    if x > 0: return 1\n",
        "\n",
        "\n",
        "def seq_delta_learning(X, Y, w, lr, iterations):\n",
        "    result = []\n",
        "    for o in range(int(iterations / len(Y))):\n",
        "        for i in range(len(Y)):\n",
        "            w_prev = w\n",
        "            x = np.concatenate((np.array([1]), X[i]))\n",
        "            t = Y[i]\n",
        "            \n",
        "            wx = np.dot(w, x)\n",
        "            y = heaviside_half(wx)\n",
        "            t_y = t - y\n",
        "            \n",
        "            update = lr * (t_y) * x\n",
        "            w = w + update\n",
        "\n",
        "            # append result\n",
        "            result.append((str(i + 1 + (len(Y) * o)), x, t, np.round(w_prev, 4),\n",
        "                           wx.round(4), y, t_y, update.round(4), w.round(4)))\n",
        "\n",
        "    return result, w\n",
        "\n",
        "def delta_learning_accuracy(X, Y, w):\n",
        "    n = len(Y)\n",
        "    true_count = 0\n",
        "    for i in range(n):\n",
        "        x = np.concatenate((np.array([1]), X[i]))\n",
        "        t = Y[i]\n",
        "\n",
        "        wx = np.dot(w, x)\n",
        "        y = heaviside_half(wx)\n",
        "        \n",
        "        if t == y:\n",
        "            true_count += 1\n",
        "            \n",
        "    return (true_count/n)\n",
        "\n",
        "# feature vector\n",
        "X = np.array([[0.0, 0.0],\n",
        "              [0.0, 0.1],\n",
        "              [1.0, 0.0],\n",
        "              [1.0, 1.0]])\n",
        "#output class\n",
        "Y = np.array([0, 0, 0, 1])\n",
        "#weights\n",
        "w = np.array([0.5, 1, 1]) \n",
        "lr = 1\n",
        "epochs = 5\n",
        "iterations = epochs * len(X)\n",
        "\n",
        "result, w_new = seq_delta_learning(X, Y, w, lr, iterations)\n",
        "\n",
        "\n",
        "# prettytable\n",
        "# -----------------------------------------------------------\n",
        "pt = PrettyTable(('iteration', 'x', 't', 'w', 'wx', 'y=H(wx)', 't-y', 'update', 'w_new'))\n",
        "for row in result: pt.add_row(row)\n",
        "\n",
        "pt.align['iteration'] = 'c'\n",
        "pt.align['x'] = 'l'\n",
        "pt.align['t'] = 'r'\n",
        "pt.align['w'] = 'l'\n",
        "pt.align['wx'] = 'l'\n",
        "pt.align['y=H(wx)'] = 'r'\n",
        "pt.align['t-y'] = 'r'\n",
        "pt.align['update'] = 'l'\n",
        "pt.align['w_new'] = 'l'\n",
        "\n",
        "print(pt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FElcBdmEXM8G"
      },
      "source": [
        "**Question 4**\n",
        "\n",
        "Repeat the above question using the batch Delta learning rule.\n",
        "Same as above for augemnted weight sign infront \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdszp9WAAWvF"
      },
      "source": [
        "import numpy as np\n",
        "from prettytable import PrettyTable\n",
        "class Perceptron:\n",
        "    def __init__(self):\n",
        "        self.weights=[]\n",
        "\n",
        "    #activation function\n",
        "    def activation(self,data):\n",
        "        #initializing with threshold value\n",
        "        activation_val=self.weights[0]\n",
        "        activation_val+=np.dot(self.weights[1:],data)\n",
        "        val=0\n",
        "        if activation_val>0: \n",
        "          val=1\n",
        "        elif activation_val==0:\n",
        "          val=0.5\n",
        "        else: \n",
        "          val=0\n",
        "        return val\n",
        "        #return 1 if activation_val>=0 else 0\n",
        "\n",
        "    def fit(self,X,y,lrate, epochs, weights):\n",
        "        #initializing weight vector\n",
        "        self.weights=weights\n",
        "        #no.of iterations to train the neural network\n",
        "        result = []\n",
        "        batchweights=weights\n",
        "        for epoch in range(epochs):\n",
        "            diffs=[]\n",
        "            for index in range(len(X)):\n",
        "                w_old=self.weights.copy()\n",
        "                x=X[index]\n",
        "                xt = np.concatenate(([1], x))\n",
        "                predicted=self.activation(x)\n",
        "                #check for misclassification\n",
        "                if (Y[index]==predicted):\n",
        "                    pass\n",
        "                else:\n",
        "                    #calculate the error value\n",
        "                    error=Y[index]-predicted\n",
        "                    #updation of threshold\n",
        "                        \n",
        "                    diffs.append(np.array(lrate*(Y[index]-predicted)*xt))\n",
        "\n",
        "                result.append((str((index+1)+(epoch)*len(X)), Y[index], np.round(w_old, 4), predicted, Y[index]-predicted, lrate*(Y[index]-predicted)*xt, np.round(self.weights, 4)))\n",
        "            diff=np.zeros(2)\n",
        "\n",
        "            for j in range(len(diffs)):\n",
        "              diff=diff+diffs[j]  \n",
        "            print('total weight', diff)\n",
        "            batchweights=batchweights+np.array(diff)\n",
        "            print('epoch', epoch,':', batchweights)\n",
        "            self.weights=batchweights\n",
        "\n",
        "        pt = PrettyTable(('iteration', 't', 'w_old', 'H(wx)', 't-y', \"(ty)x\", 'w_new'))\n",
        "        for row in result: pt.add_row(row)\n",
        "\n",
        "        pt.align['iteration'] = 'c'\n",
        "        pt.align['t'] = 'l'\n",
        "        pt.align['w_old'] = 'l'\n",
        "        pt.align['H(wx)'] = 'l'\n",
        "        pt.align['t-y'] = 'l'\n",
        "        pt.align['(ty)x'] = 'l'\n",
        "        pt.align['w_new'] = 'l'\n",
        "\n",
        "        print(pt)\n",
        "\n",
        "\n",
        "    #training perceptron for the given data\n",
        "    def predict(self,x_test):\n",
        "        predicted=[]\n",
        "        for i in range(len(x_test)):\n",
        "            #prediction for test set using obtained weights\n",
        "            predicted.append(self.activation(x_test.iloc[i]))\n",
        "        return predicted\n",
        "    \n",
        "    def accuracy(self,predicted,original):\n",
        "        correct=0\n",
        "        lent=len(predicted)\n",
        "        for i in range(lent):\n",
        "            if(predicted[i]==original.iloc[i]):\n",
        "                correct+=1\n",
        "        return (correct/lent)*100\n",
        "\n",
        "    def getweights(self):\n",
        "        return self.weights\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#input data\n",
        "X=[[0],  [1]]\n",
        "#output class\n",
        "Y=[1, 0]\n",
        "#splitting test and train data for iris\n",
        "model=Perceptron()\n",
        "#weights but sign is important for weight bias \n",
        "init_weights=[-1.5, 2]   \n",
        "\n",
        "l_rate=1\n",
        "epoch=7\n",
        "model.fit(X, Y, l_rate, epoch, init_weights)\n",
        "\n",
        "#if x^t column comes up it's just a input vector with 1 infront (augmented input vector)\n",
        "# t is just a true output label\n",
        "# total weight change i.e. first epoch between intial weights and new weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfsOgkUjqv5A"
      },
      "source": [
        "#Ngeative feedback\n",
        "**Question 8**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX7DzlYuDCZr"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def activationOfOutput(weights, iterations, input, alpha, activations):\n",
        "\n",
        "    prev_activations = activations\n",
        "    iteration = 1\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        print(\"Iteration {}\".format(iteration))\n",
        "        \n",
        "        # Following block deals with calculating first equation: e = x - W^{T}y\n",
        "        wT = np.array(weights).T\n",
        "        wTy = np.dot(wT, activations)\n",
        "        print(\"value of wTy {}\".format(wTy))\n",
        "\n",
        "        eT = input - wTy\n",
        "        print(\"e^t: {}\".format(eT))\n",
        "        e = np.array(eT).reshape((3, 1))\n",
        "\n",
        "        # The following lines deal with calculating the update: y <- y + \\alpha*W*e\n",
        "        We = np.dot(weights, e)\n",
        "        We = [j for i in We for j in i]\n",
        "        print(\"(We)^t: {} \".format(We))\n",
        "\n",
        "        alphaWe = np.dot(alpha, We)\n",
        "\n",
        "        # Doing the actual update using the second equation:\n",
        "        y = activations + alphaWe\n",
        "        print(\"Value of y^t: {}\\n\".format(y))\n",
        "\n",
        "        activations = y\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    print(\"\\nAfter {} iterations, the activation of the output neurons is equal to {}\".format(iterations, activations))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # Change these parameters depending on what the question asks for:\n",
        "    activationOfOutput(weights = np.array([[1, 1], [1, 1], [0, 1]]).T, iterations = 5, input = [1, 1, 0], alpha = 0.25, activations = [0, 0])\n",
        "\n",
        "    #activationOfOutput(weights = [[1, 1, 0], [1, 1, 1]], iterations = 5, input = [1, 1, 0], alpha = 0.5, activations = [0, 0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESPvGN7lyMB9"
      },
      "source": [
        "##batch normalisation\n",
        "**Question 5 Week 5 Spartling section**\n",
        "\n",
        "The following arrays show the output produced 2 by a convolutional layer to all 4 samples in a batch.\n",
        "\n",
        "Calculate the corresponding outputs produced after the application of batch normalisation, assuming the following\n",
        "parameter values \f beta= 0, \n",
        "agamme= 1, and \u000f alpha= 0:1 which are the same for all neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5t-vEAlyAm4"
      },
      "source": [
        "#Question 5 Week 5\n",
        "import numpy as np \n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "b=0\n",
        "c=1\n",
        "e=0.1\n",
        "x1 = np.array([[1,0.5,0.2], \n",
        "               [-1,-0.5,-0.2],\n",
        "               [0.1,-0.1,0]])\n",
        "\n",
        "x2 = np.array([[1,-1,0.1], \n",
        "               [0.5,-0.5,-0.1],\n",
        "               [0.2,-0.2,0]])\n",
        "\n",
        "x3 = np.array([[0.5,-0.5,-0.1], [0,-0.4,0],[0.5,0.5,0.2]])\n",
        "\n",
        "x4 = np.array([[0.2,1,-0.2], [-1,-0.6,-0.1],[0.1,0,0.1]])\n",
        "\n",
        "m =  (x1 + x2 + x3 + x4 ) / 4.0\n",
        "print ('the median is:',m, '\\n')\n",
        "\n",
        "v =  ((x1-m)**2 + (x2-m)**2 + (x3-m)**2 + (x4-m)**2) / 4.0\n",
        "print ('the variance is:',v, '\\n')\n",
        "\n",
        "Bn1=b+c*(x1-m)/(np.sqrt(v+e))\n",
        "print('the batch normalization of x1:',  Bn1, '\\n')\n",
        "\n",
        "Bn2=b+c*(x2-m)/(np.sqrt(v+e))\n",
        "print('the batch normalization of x2:', Bn2, '\\n')\n",
        "\n",
        "Bn3=b+c*(x3-m)/(np.sqrt(v+e))\n",
        "print('the batch normalization of x3:', Bn3, '\\n')\n",
        "\n",
        "Bn4=b+c*(x4-m)/(np.sqrt(v+e))\n",
        "print('the batch normalization of x3:',  Bn4, '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJlc38zVkxgy"
      },
      "source": [
        "**Applying tanh activation function on a net matrix**\n",
        "##Question 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59iNDU1-k47Q",
        "outputId": "8990b87e-e99c-405e-97b6-047b9c89401a"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "x = np.array ([[1, 0.5, 0.2],\n",
        "               [-1, -0.5, -0.2],\n",
        "               [0.1, -0.1, 0]])\n",
        "\n",
        "tanh_values = np.tanh(x)\n",
        "print(tanh_values, '\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.76159416  0.46211716  0.19737532]\n",
            " [-0.76159416 -0.46211716 -0.19737532]\n",
            " [ 0.09966799 -0.09966799  0.        ]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXNII-l_v0J7"
      },
      "source": [
        "##question 6\n",
        "\n",
        "following arrays show the feature maps that provide the input to a convolutional layer of a CNN.\n",
        "\n",
        "Calculate the output produced by mask H when using:\n",
        "<br>\n",
        "a. padding=0, stride=1\n",
        "<br>\n",
        "b. padding=1, stride=1\n",
        "<br>\n",
        "c. padding=1, stride=2\n",
        "<br>\n",
        "d. padding=0, stride=1, dilation=2\n",
        "\n",
        "\n",
        "##for output check all paddicng and stride and diliation as question asked for\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFJWOs6GtsK4"
      },
      "source": [
        "#week 5\n",
        "import numpy as np\n",
        "\n",
        "# %% Applying masks to produce feature mask\n",
        "\n",
        "#mask #kernal #filter\n",
        "H = np.array(\n",
        "    [\n",
        "        [\n",
        "            [1, -0.1],\n",
        "            [1, -0.1],\n",
        "        ],\n",
        "        [\n",
        "            [0.5, 0.5],\n",
        "            [-0.5, -0.5],\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "#feature matrix #input matrix\n",
        "x = np.array(\n",
        "    [\n",
        "        [\n",
        "            [0.2, 1, 0],\n",
        "            [-1, 0, -0.1],\n",
        "            [0.1, 0, 0.1],\n",
        "        ],\n",
        "        [\n",
        "            [1, 0.5, 0.2],\n",
        "            [-1, -0.5, -0.2],\n",
        "            [0.1, -0.1, 0],\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def strided_len(x_len, H_len, stride):\n",
        "    return np.ceil((x_len - H_len + 1) / stride).astype(int)\n",
        "\n",
        "\n",
        "def H_dilated_len(H_len, dilation):\n",
        "    return (H_len - 1) * (dilation - 1) + H_len\n",
        "\n",
        "\n",
        "def dilate_H(H, dilation):\n",
        "    H_rows, H_cols = H[0].shape\n",
        "    H_dilated = np.zeros((H.shape[0], H_dilated_len(H_rows, dilation), H_dilated_len(H_cols, dilation)))\n",
        "    H_dilated[:, ::dilation, ::dilation] = H\n",
        "    return H_dilated\n",
        "\n",
        "\n",
        "def apply_mask(x, H, padding=0, stride=1, dilation=1):\n",
        "    # x and H can have multiple channels in the 0th dimension\n",
        "    if padding > 0:\n",
        "        x = np.pad(x, pad_width=padding, mode='constant')[1:-1]\n",
        "\n",
        "    if dilation > 1:\n",
        "        H = dilate_H(H, dilation)\n",
        "\n",
        "    H_rows, H_cols = H[0].shape\n",
        "    x_rows, x_cols = x[0].shape\n",
        "\n",
        "    fm_rows, fm_cols = strided_len(x_rows, H_rows, stride), strided_len(x_cols, H_cols, stride)\n",
        "    feature_map = np.empty((fm_rows, fm_cols))\n",
        "    for xf in range(fm_rows):\n",
        "        for yf in range(fm_cols):\n",
        "            xi, yi = xf * stride, yf * stride\n",
        "            receptive_region = x[:, xi : xi + H_rows, yi : yi + H_cols]\n",
        "            feature_map[xf, yf] = np.sum(H * receptive_region)\n",
        "    return feature_map\n",
        "\n",
        "\n",
        "##for output check all paddicng and stride and diliation as question asked for\n",
        "\n",
        "print(apply_mask(x, H), '\\n') \n",
        "print(apply_mask(x, H, padding=1), '\\n')\n",
        "print(apply_mask(x, H, padding=1, stride=2), '\\n')\n",
        "print(apply_mask(x, H, padding=0, stride=1, dilation=2), '\\n')\n",
        "\n",
        "# %% 1x1 convolution\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8uKxo-jzL6s"
      },
      "source": [
        "##Question 7\n",
        "\n",
        "The following arrays show the feature maps that provide the input to a convolutional layer of a CNN.\n",
        "\n",
        "Calculate the output produced by 1x1 convolution when the 3 channels of the 1x1 mask are: [1, -1, 0.5]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkBMpT2tuUPq",
        "outputId": "37469e24-d713-44d4-d062-d5762384dfd9"
      },
      "source": [
        "#week 5\n",
        "import numpy as np\n",
        "x = np.array(\n",
        "    [\n",
        "        [\n",
        "            [0.2, 1, 0],\n",
        "            [-1, 0, -0.1],\n",
        "            [0.1, 0, 0.1],\n",
        "        ],\n",
        "        [\n",
        "            [1, 0.5, 0.2],\n",
        "            [-1, -0.5, -0.2],\n",
        "            [0.1, -0.1, 0],\n",
        "        ],\n",
        "        [\n",
        "            [0.5, -0.5, -0.1],\n",
        "            [0, -0.4, 0],\n",
        "            [0.5, 0.5, 0.2],\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "#feature#mask#filter\n",
        "H = np.array([[[1]], [[-1]], [[0.5]]])\n",
        "\n",
        "apply_mask(x, H)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.55,  0.25, -0.25],\n",
              "       [ 0.  ,  0.3 ,  0.1 ],\n",
              "       [ 0.25,  0.35,  0.2 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLpjCuLUzngs"
      },
      "source": [
        "**Question 8**\n",
        "\n",
        "The following array shows the input to a pooling layer of a CNN.\n",
        "\n",
        "Calculate the output produced by the pooling when using:\n",
        "<br>\n",
        "a. average pooling with a pooling region of 2x2 and stride=2\n",
        "<br>\n",
        "b. max pooling with a pooling region of 2x2 and stride=2\n",
        "<br>\n",
        "c. max pooling with a pooling region of 3x3 and stride=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGwYJKozvG9U"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "x = np.array(\n",
        "    [\n",
        "        [\n",
        "            [0.2, 1, 0, 0.4],\n",
        "            [-1, 0, -0.1, -0.1],\n",
        "            [0.1, 0, -1, -0.5],\n",
        "            [0.4, -0.7, -0.5, 1],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def strided_len(x_len, H_len, stride):\n",
        "    return np.ceil((x_len - H_len + 1) / stride).astype(int)\n",
        "\n",
        "\n",
        "def pooling(x, pooling_function, region=(2, 2), stride=1):\n",
        "    x_rows, x_cols = x[0].shape\n",
        "    pool_rows, pool_cols = strided_len(x_rows, region[0], stride), strided_len(x_cols, region[1], stride)\n",
        "    pool = np.empty((x.shape[0], pool_rows, pool_cols))\n",
        "    for xp in range(pool_rows):\n",
        "        for yp in range(pool_cols):\n",
        "            xi, yi = xp * stride, yp * stride\n",
        "            pooling_region = x[:, xi : xi + pool_rows, yi : yi + pool_cols]\n",
        "            pool[:, xp, yp] = pooling_function(pooling_region)\n",
        "    return pool\n",
        "\n",
        "#average, different filter/mask size will be asked (2,2) here \n",
        "print(pooling(x, np.mean, (2, 2), stride=2), '\\n')\n",
        "#max with stride 2\n",
        "print(pooling(x, np.max, (2, 2), stride=2), '\\n')\n",
        "#max with stride 1\n",
        "print(pooling(x, np.max, (3, 3), stride=1), '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSIZMJckvwXL"
      },
      "source": [
        "##Week 7\n",
        "<br>\n",
        "QUESTION 7\n",
        "Apply two epochs of a batch version of Ojas learning rule to the same data used in the previous question. Use\n",
        "a learning rate of 0.01 and an initial weight vector of [-1,0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T0b6fG9E-E3"
      },
      "source": [
        "#oja rule\n",
        "#week 7\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "input = np.array([[0,1],[3,5],[5,4],[5,6],[8,7],[9,7]])\n",
        "w=np.array([-1,0])\n",
        "lr=0.01\n",
        "\n",
        "x_mean=0\n",
        "y_mean=0\n",
        "for n in input:\n",
        "    x_mean=x_mean+n[0]\n",
        "    y_mean=y_mean+n[1]\n",
        "mean=[x_mean/len(input),y_mean/len(input)]\n",
        "print(mean, 'mean')\n",
        "\n",
        "#table = []\n",
        "time =np.array([-1,0]).T\n",
        "for i in range (len(input)):\n",
        "    input[i] = input[i]-mean\n",
        "    #table.append(input[i])\n",
        "    #table_array = np.array(table)\n",
        "    t = np.dot(input[i], w)\n",
        "    print(t)\n",
        "    \n",
        "print(input, '\\n')\n",
        "\n",
        "i=0\n",
        "n=2\n",
        "while i<n:\n",
        "    change = []\n",
        "    for x in input:\n",
        "        y=np.dot(w,x)\n",
        "        change.append(lr*y*(x-y*w))\n",
        "    total = np.array([0,0])\n",
        "    for x in change:\n",
        "        total=total+x\n",
        "    w = w+total\n",
        "\n",
        "    print(\"change\",change, '\\n')\n",
        "    print(\"total change\",total,\"new w:\",w, '\\n')\n",
        " \n",
        "    i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGbe4S8a3vrU"
      },
      "source": [
        "##Ignore KLT\n",
        "#matlab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qut44AXd5unf"
      },
      "source": [
        "#fisher method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jydjvpoe5EUX",
        "outputId": "b895f41c-b8f0-4124-d090-9cc59507e997"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def fishers(ip, weights, classes):\n",
        "    ip = np.array(ip)\n",
        "    N, D = ip.shape\n",
        "    weights = np.array(weights)\n",
        "    m1 = []\n",
        "    m2 = []\n",
        "    for idx in range(N):\n",
        "        if classes[idx] == 1:\n",
        "            m1.append(ip[idx])\n",
        "        else:\n",
        "            m2.append(ip[idx])\n",
        "    m1 = np.mean(m1, axis=0)\n",
        "    m2 = np.mean(m2, axis=0)\n",
        "\n",
        "    # between cluster distance\n",
        "    sb = []\n",
        "    sw = []\n",
        "    for w in (weights):\n",
        "        d = (w @ (m1-m2)) ** 2\n",
        "        sb.append(d)\n",
        "    # calculate within cluster distance\n",
        "    sw = []\n",
        "    for w in weights:\n",
        "        running_sw = 0\n",
        "        for idx in range(len(ip)):\n",
        "            if classes[idx] == 1:\n",
        "                running_sw += (w.T @ (ip[idx] - m1)) ** 2\n",
        "\n",
        "            elif classes[idx] == 2:\n",
        "                running_sw += (w.T @ (ip[idx] - m2)) ** 2\n",
        "        sw.append(running_sw)\n",
        "        # print(running_sw)\n",
        "    print(\"SB: \")\n",
        "    print(sb)\n",
        "    print(\"SW: \")\n",
        "    print(sw)\n",
        "    cost = []\n",
        "    for _sb, _sw in zip(sb, sw):\n",
        "        cost.append(_sb/_sw)\n",
        "    print(\"Cost: \")\n",
        "    print(cost)\n",
        "\n",
        "    print(\"-\"*100)\n",
        "    print(f\"{weights[np.argmax(cost)]} has high PROJECTION COST\")\n",
        "\n",
        "\n",
        "ip = [[1, 2], [2, 1], [3, 3], [6, 5], [7, 8]]\n",
        "classes = [1, 1, 1, 2, 2]\n",
        "weights = [[-1, 5], [2, -3]]\n",
        "fishers(ip, weights, classes)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SB: \n",
            "[324.0, 20.25]\n",
            "SW: \n",
            "[140.0, 38.5]\n",
            "Cost: \n",
            "[2.3142857142857145, 0.525974025974026]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[-1  5] has high PROJECTION COST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFQbTUOm7ECX"
      },
      "source": [
        "##Sparse coding\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDv2xSwy-fXG",
        "outputId": "8fcf6310-e0c6-49d7-90d8-23785ae592eb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#replace all vectors based on question\n",
        "\n",
        "Vt=np.array([[0.4, 0.55, 0.5, -0.1, -0.5, 0.9, 0.5, 0.45], [-0.6, -0.45, -0.5, 0.9, -0.5, 0.1, 0.5, 0.55]])\n",
        "y1=np.array([1,0,0,0,1,0,0,0])\n",
        "y2=np.array([0,0,1,0,0,0,-1,0])\n",
        "x=np.array([[-0.05, -0.95]])\n",
        "error1=x-np.dot(Vt,y1)\n",
        "error2=x-np.dot(Vt,y2)\n",
        "print('error1', error1, '\\n' 'error2', error2)\n",
        "print(np.linalg.norm(error1), 'first error ')\n",
        "print(np.linalg.norm(error2), 'second error ')\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error1 [[0.05 0.15]] \n",
            "error2 [[-0.05  0.05]]\n",
            "0.15811388300841908 first error \n",
            "0.0707106781186548 second error \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}