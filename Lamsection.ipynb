{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lamsection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzV/hrIYNTEopavm2Sx3QJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashyibo23/pattern-recognition-/blob/main/Lamsection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv-zikOxDfok"
      },
      "source": [
        "##Week 4 Neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dsH9Xb3Z7hr"
      },
      "source": [
        "**Question 6 Radial Basis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShgvnCp1Dbrg",
        "outputId": "69c9ad88-8214-4189-cd2f-efee46fcade1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# --------------------------------INPUT--------------------------------#\n",
        "X = np.array(\n",
        "    [\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ]\n",
        ")\n",
        "\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "#cluster center it will be given in the question \n",
        "C = np.array(\n",
        "    [\n",
        "        [0, 0],\n",
        "        [1, 1],\n",
        "    ]\n",
        ")\n",
        "# ----------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "# ---------------------------------TEST---------------------------------#\n",
        "#input pattern for the XOR classifier\n",
        "#predicting input data\n",
        "X_predict = np.array(\n",
        "    [\n",
        "        [0.5, -0.1],\n",
        "        [-0.2, 1.2],\n",
        "        [0.8, 0.3],\n",
        "        [1.8, 0.6]\n",
        "    ]\n",
        ")\n",
        "# ----------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "# --------------Calculate rho and sigma for radial basis---------------#\n",
        "center_dist = []\n",
        "for i in range(0, len(C) - 1):\n",
        "    for j in range(i + 1, len(C)):\n",
        "        print(i, j)\n",
        "        center_dist.append(np.sqrt(np.sum((C[i, :] - C[j, :]) ** 2)))\n",
        "\n",
        "rho_max = np.max(center_dist)\n",
        "rho_avg = np.average(center_dist)\n",
        "nH = len(C)\n",
        "print(\"Rho max: \", rho_max)\n",
        "print(\"Rho average: \", rho_avg)\n",
        "\n",
        "#nH is depends on the number of cluster/hidden units in the hidden layer\n",
        "\n",
        "sigma = rho_max / np.sqrt(2 * nH)       # Using rho-max\n",
        "# sigma = 2 * rho_avg                   # Using rho-average\n",
        "print(\"Sigma: \", sigma)\n",
        "\n",
        "\n",
        "# -----------------------------CALCULATE OUTPUTS---------------------------#\n",
        "\n",
        "def get_hidden_output(X, C, sigma):\n",
        "    radial_basis_output = []\n",
        "    for i in range(0, len(X)):\n",
        "        hidden_node_outputs = []\n",
        "        for j in range(0, len(C)):\n",
        "            # Using Gaussian function\n",
        "            hidden_node_outputs.append(np.exp(-np.sum((X[i] - C[j]) ** 2)) / (2 * sigma * sigma))\n",
        "        radial_basis_output.append(hidden_node_outputs)\n",
        "\n",
        "    print(\"Radial basis layer output rounded by 2dp: \")\n",
        "    print(np.round(radial_basis_output, 2))\n",
        "    return radial_basis_output\n",
        "\n",
        "# Get output from hidden rbf layer\n",
        "radial_basis_output = get_hidden_output(X, C, sigma)\n",
        "\n",
        "# Add bias to hidden layer output\n",
        "radial_basis_output = np.c_[radial_basis_output, np.ones(len(radial_basis_output))]\n",
        "radial_basis_output_transposed = np.transpose(radial_basis_output)\n",
        "\n",
        "# Least squares method to calculate weights\n",
        "weights = np.dot(\n",
        "    np.dot(\n",
        "        np.linalg.inv(\n",
        "            np.dot(radial_basis_output_transposed, radial_basis_output)\n",
        "        ), radial_basis_output_transposed\n",
        "    ), Y\n",
        ")\n",
        "print(\"Weights between hidden-output layer: \", np.round(weights, 2))\n",
        "\n",
        "# Get output of final layer\n",
        "calculated_output = np.dot(radial_basis_output, weights)\n",
        "print(\"Output of network: \", np.round(calculated_output, 2))\n",
        "\n",
        "# Apply basic sign function with 0.5 threshold or call any other activation function\n",
        "final_output = np.where(calculated_output > 0.5, 1, 0)\n",
        "print(\"Signed output of network: \", final_output)\n",
        "\n",
        "# ---------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "# ------------------------------PREDICT TEST OUTPUT---------------------------#\n",
        "\n",
        "# Get output of hidden rbf layer \n",
        "hidden_output = get_hidden_output(X_predict, C, sigma)\n",
        "\n",
        "# Get output of final output layer\n",
        "predicted_output = np.dot(np.c_[hidden_output, np.ones(len(hidden_output))], weights)\n",
        "print(\"Output of test samples: \", np.round(predicted_output, 2))\n",
        "\n",
        "test_output = np.where(predicted_output > 0.5, 1, 0)\n",
        "print(\"Signed Output of test samples: \", test_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1\n",
            "Rho max:  1.4142135623730951\n",
            "Rho average:  1.4142135623730951\n",
            "Sigma:  0.7071067811865476\n",
            "Radial basis layer output: \n",
            "[[1.   0.14]\n",
            " [0.37 0.37]\n",
            " [0.37 0.37]\n",
            " [0.14 1.  ]]\n",
            "Weights between hidden-output layer:  [-2.5  -2.5   2.84]\n",
            "Output of network:  [-0.  1.  1.  0.]\n",
            "Signed output of network:  [0 1 1 0]\n",
            "Radial basis layer output: \n",
            "[[0.77 0.23]\n",
            " [0.23 0.23]\n",
            " [0.48 0.59]\n",
            " [0.03 0.45]]\n",
            "Output of test samples:  [0.33 1.7  0.16 1.65]\n",
            "Signed Output of test samples:  [0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuRfPHo7d3xl"
      },
      "source": [
        "##GANS\n",
        "**Question 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71SfF7RNq7Pr"
      },
      "source": [
        "import math as math\n",
        "import sympy as sy\n",
        "\n",
        "#\n",
        "# NOTE: This script only works for k=1. Examples with different values\n",
        "#       of k are not provided in the lecture or the tutorial.\n",
        "#\n",
        "\n",
        "# Change this variables according to the question\n",
        "training_iterations = 1 # Number of training iterations.\n",
        "x_real = [[1,2],[3,4]] # Real samples\n",
        "x_fake = [[5,6],[7,8]] # Generated samples\n",
        "    # Both x_real and x_fake must have the same number of \n",
        "    # samples for part b to work\n",
        "\n",
        "theta = [0.1,0.2] # Initial parameters of discriminator\n",
        "\n",
        "prob_real = 0.5 # Probability of real samples to be selected\n",
        "prob_fake = 0.5 # Probability of generated samples to be selected\n",
        "    # In Tutorial 7 Q.3 each sample has the same probability,\n",
        "    # that is why the current value is 0.5 for both\n",
        "\n",
        "learning_rate = 0.02 # Part b Q.3 says that learning rate is 0.02 but\n",
        "                    # it can be changed to the desired value\n",
        "\n",
        "# DO NOT change this variables\n",
        "# ----------------------------\n",
        "# Here, symbols are created for each variable so that\n",
        "# the discriminator function can be differentiated and\n",
        "# evaluated with different values\n",
        "n = len(x_real[0]) \n",
        "x = [None] * n\n",
        "t = [None] * n\n",
        "for i in range(n):\n",
        "    name = 'x'+str(i)\n",
        "    x[i] = sy.Symbol(name)\n",
        "    name = 't'+str(i)\n",
        "    t[i] = sy.Symbol(name)\n",
        "# ----------------------------\n",
        "\n",
        "# Change this according to the problem's discriminator function\n",
        "discriminator_function = 1/(1+ math.e**-(t[0]*x[0] - t[1]*x[1] - 2))\n",
        "    # The discriminator function can be changed to any function needed.\n",
        "    # \n",
        "    # Note that the variables MUST be expressed as t[i] and x[i] \n",
        "    # where:\n",
        "    # 't' is a list containig variables for the parameters of Discriminator (Theta)\n",
        "    # 'x' is a list containig variables for each attribute of each sample\n",
        "    # 'i' is the number of the attribute and parameter used in that case\n",
        "    # \n",
        "    # i.e. for the equation: \"θ1*x1\" \n",
        "    # then above it should be written: \"t[0]*x[0]\"\n",
        "    # because python lists start in 0\n",
        "\n",
        "# *****DO NOT CHANGE CODE BELOW THIS LINE********\n",
        "#\n",
        "m = len(x_real)\n",
        "m_fake = len(x_fake)\n",
        "\n",
        "#\n",
        "# Because it is discrete we don't need to find integrals here when computing expectations\n",
        "#\n",
        "print('************')\n",
        "print('*****GAN****')\n",
        "print('************')\n",
        "\n",
        "#\n",
        "# Tutorial 7 Question 3 part a\n",
        "#\n",
        "for iter in range(training_iterations):\n",
        "    print ('\\n--Start of',str(iter+1)+\"°\",'training iteration.--')\n",
        "    V_D = 0 # Discriminator Value \n",
        "\n",
        "    # This for is to obtain the discriminator value\n",
        "    for i in range(m):\n",
        "        xx = x_real[i]\n",
        "        res_discr_funct = discriminator_function\n",
        "        for j in range(n):\n",
        "            # .subs() is a function that replaces the x[i] and t[i] variables of the \n",
        "            # discriminant function with the actual values of the theta and real samples\n",
        "            # given in the problem\n",
        "            res_discr_funct = res_discr_funct.subs(x[j],xx[j]).subs(t[j],theta[j])\n",
        "        V_D += prob_real*math.log(res_discr_funct)\n",
        "\n",
        "\n",
        "    print('\\n***** PART A ****\\n')\n",
        "\n",
        "    print('\\nThe Discriminator value is', V_D)\n",
        "\n",
        "    V_G = 0 # Generator Value\n",
        "\n",
        "    # This for is to obtain the generator value\n",
        "    for i in range(m_fake):\n",
        "        xx = x_fake[i]\n",
        "        res_discr_funct = discriminator_function\n",
        "        for j in range(n):\n",
        "            # .subs() is a function that replaces the x[i] and t[i] variables of the \n",
        "            # discriminant function with the actual values of the theta and real samples\n",
        "            # given in the problem\n",
        "            res_discr_funct = res_discr_funct.subs(x[j],xx[j]).subs(t[j],theta[j])\n",
        "        V_G += prob_fake*math.log(1 - res_discr_funct)\n",
        "        \n",
        "    print('\\nThe Generator value is', V_G)\n",
        "\n",
        "    V_DG = V_D + V_G\n",
        "\n",
        "    print('\\nThe Computed V_DG is ', V_DG)\n",
        "\n",
        "    #\n",
        "    # Tutorial 7 Question 3 part b\n",
        "    #\n",
        "\n",
        "    print('\\n\\n*********************')\n",
        "    print('***** PART B ****\\n')\n",
        "\n",
        "    alpha_beta = [[0] * n] * m\n",
        "    for i in range(m):\n",
        "        xx = x_real[i]\n",
        "        xx_bar = x_fake[i]\n",
        "        # Two different equations are needed. One with x_real[i] and another with\n",
        "        # x_fake[i]. This is explained in the tutorial solutions.\n",
        "        discriminator_function_xx = discriminator_function\n",
        "        discriminator_function_xx_bar = discriminator_function\n",
        "        for j in range(n):\n",
        "            discriminator_function_xx = discriminator_function_xx.subs(x[j],xx[j])\n",
        "            discriminator_function_xx_bar = discriminator_function_xx_bar.subs(x[j],xx_bar[j])\n",
        "\n",
        "        learning_equation = sy.log(discriminator_function_xx) \n",
        "        learning_equation += sy.log(1 - discriminator_function_xx_bar)\n",
        "        \n",
        "        differential_equation = [None] * n\n",
        "        # The learning equation is differentiated in this for\n",
        "        for j in range(n):\n",
        "            differential_equation[j] = learning_equation.diff(t[j])\n",
        "            # After differentiating the learning equation, the next for replaces\n",
        "            # thetha variables with the actual values to obtain the result\n",
        "            for k in range(n):\n",
        "                differential_equation[j] = differential_equation[j].subs(t[k],theta[k])\n",
        "        # The result of the differentiated equations is added to a list\n",
        "        alpha_beta[i] = differential_equation\n",
        "\n",
        "    # Alpha and beta are added together to obtain delta\n",
        "    delta_theta = [0] * n\n",
        "    for i in range(n):\n",
        "        delta_theta[i] = 0\n",
        "        for j in range(m):\n",
        "            print( 'Alpha and Beta', i+1,j+1, 'is', alpha_beta[j][i])\n",
        "            delta_theta[i] += (1/m)*alpha_beta[j][i]\n",
        "\n",
        "    print( '\\nDelta is ', delta_theta,'\\n')\n",
        "\n",
        "    # Lastly, delta is added to the current theta values to obtain the new\n",
        "    # theta values after training\n",
        "    for i in range(n):\n",
        "        theta[i] = theta[i] + learning_rate*delta_theta[i]\n",
        "        print('New theta',i+1   ,'is',theta[i])\n",
        "    print ('\\n--End of',str(iter+1)+\"°\",'training iteration.--')\n",
        "print ('\\n\\n---------End of the script.---------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8XTX2N0fekc"
      },
      "source": [
        "##SVM\n",
        "**Question 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq1suj-xeVgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46aaf8a6-dd52-4b22-d74d-5c30157baaa9"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True) \n",
        "\n",
        "\n",
        "def _svm(X, y, support_vectors, support_vector_class):\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(\"-\"*100)\n",
        "    # support_vectors = np.array([[3, 1], [3, -1], [1, 0]])\n",
        "    # support_vector_class = np.array([1, 1, -1])\n",
        "    w = []\n",
        "    for idx in range(len(support_vectors)):\n",
        "        w.append(support_vectors[idx] * support_vector_class[idx])\n",
        "    w = np.array(w)\n",
        "    eq_arr = []\n",
        "    for idx, sv in enumerate(support_vectors):\n",
        "        tmp = ((w @ sv) * support_vector_class[idx])\n",
        "        tmp = np.append(tmp, [support_vector_class[idx]])\n",
        "        eq_arr.append(tmp)\n",
        "    eq_arr.append(np.append(support_vector_class, [0]))\n",
        "    rhs_arr = [1] * len(support_vector_class)\n",
        "    rhs_arr.extend([0])\n",
        "    rhs_arr = np.array(rhs_arr)\n",
        "    try:\n",
        "        ans = rhs_arr @ np.linalg.inv(eq_arr)\n",
        "    except:\n",
        "        print(\"Unabelt to do inverse, taking pseudo inverse\")\n",
        "        ans = rhs_arr @ np.linalg.pinv(eq_arr)\n",
        "    print(\"lambda and w_0 values are \", ans)\n",
        "    final_weight = []\n",
        "    for idx in range(w.shape[0]):\n",
        "        final_weight.append(w[idx] * ans[idx])\n",
        "    final_weight = np.array(final_weight)\n",
        "    final_weight = np.sum(final_weight, axis=0)\n",
        "    print(\"Weights: \")\n",
        "    print(final_weight)\n",
        "    print(\"Margin: \")\n",
        "    print(2/np.linalg.norm(final_weight))\n",
        "    print(\"-\"*100)\n",
        "\n",
        "\n",
        "# REPLACE X AND Y ACCORDING TO THE QUESTION\n",
        "# REPLACE \"support_vectors\" and \"support_vector_class\"\n",
        "X = [[3, 1], [3, -1], [7, 1], [8, 0], [1, 0], [0, 1], [-1, 0], [-2, 0]]\n",
        "\n",
        "#depends on the exam question needs to change class label y\n",
        "y = [1, 1, 1, 1, -1, -1, -1, -1]\n",
        "\n",
        "#here it's given support vectors but needs to change based on exam question if support vectors\n",
        "support_vectors = np.array(\n",
        "    [[3, 1], [3, -1], [1, 0]])\n",
        "support_vector_class = np.array([1, 1, -1])\n",
        "# support_vectors = np.array([[3, 3], [-1, -2]])\n",
        "# support_vector_class = np.array([1, -1])\n",
        "# X = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])\n",
        "# y = np.array([1, 1, -1, -1])\n",
        "# support_vectors = X\n",
        "# support_vector_class = y\n",
        "_svm(X, y, support_vectors=support_vectors,\n",
        "     support_vector_class=support_vector_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "lambda and w_0 values are  [ 0.25  0.25  0.5  -2.  ]\n",
            "Weights: \n",
            "[1. 0.]\n",
            "Margin: \n",
            "2.0\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDKugVjXyIwo"
      },
      "source": [
        "##Adaboost "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfaszg7ooLvD",
        "outputId": "052df080-160b-4b63-a75b-529546acbcde"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import copy\n",
        "\n",
        "\n",
        "def main(ip, targets, heaviside_function_thresholds, heaviside_function_classes):\n",
        "    ip = np.array(ip)\n",
        "    targets = np.array(targets)\n",
        "    heaviside_function_thresholds = np.array(heaviside_function_thresholds)\n",
        "    k_max = heaviside_function_thresholds.shape[0]\n",
        "    N, D = ip.shape\n",
        "    weights = [1/N] * N\n",
        "    weights = np.array(weights)\n",
        "\n",
        "    k = 0\n",
        "    while k < k_max:\n",
        "        summary_dict = {}\n",
        "        for idx, hf in enumerate(heaviside_function_thresholds):\n",
        "            tmp = []\n",
        "            for idy, x in enumerate(ip):\n",
        "                if x[0] > hf:\n",
        "                    tmp.append(heaviside_function_classes[idx])\n",
        "                else:\n",
        "                    tmp.append(-1 * heaviside_function_classes[idx])\n",
        "\n",
        "            summary_dict[f\"C{idx+1}\"] = copy.deepcopy(tmp)\n",
        "            error_rate = 1 - accuracy_score(targets, tmp)\n",
        "            summary_dict[f\"C{idx+1}\"].append(error_rate)\n",
        "\n",
        "        if k > 0:\n",
        "            for key, arr in summary_dict.items():\n",
        "                tmp = np.array(arr[:-1])\n",
        "\n",
        "                weighted_training_error = 0\n",
        "                for idx, b in enumerate(list(targets == tmp)):\n",
        "                    if not b:\n",
        "                        weighted_training_error += (weights[idx])\n",
        "                summary_dict[key][-1] = weighted_training_error\n",
        "        print()\n",
        "        summary_df = pd.DataFrame(summary_dict).T\n",
        "        print(summary_df)\n",
        "        columns = summary_df.columns\n",
        "        epsilon = summary_df[columns[-1]].min()\n",
        "        if epsilon > 0.5:\n",
        "            k_max -= 1\n",
        "        # CALCULATING ALPHA\n",
        "        mid_term = (1 - epsilon) / epsilon\n",
        "        alpha = 0.5 * np.log(mid_term)\n",
        "        # COMPUTE WEIGHTS FOR NEXT ROUND\n",
        "        for idz in range(len(weights)):\n",
        "            e_power = -(alpha * targets[idz] *\n",
        "                        summary_df.loc[f\"C{k+1+1}\"][idz])\n",
        "            weights[idz] = (weights[idz] * (np.e ** e_power))\n",
        "        weights /= weights.sum()\n",
        "        print(\"WEIGHTS ARE: \")\n",
        "        print(weights)\n",
        "        print(\n",
        "            f\" alpha part Classified for this iteration is (log applying) {alpha}*C{summary_df[columns[-1]].argmin()+1}\")\n",
        "        user_input = input(\"Next Iteration(y/n)\").strip()\n",
        "        if user_input.lower() == \"y\":\n",
        "            k += 1\n",
        "        else:\n",
        "            k = k_max\n",
        "\n",
        "#input vector \n",
        "ip = [[1, 0], [-1, 0], [0, 1], [0, -1]]\n",
        "#for each input vector it's output class 1 or -1\n",
        "targets = [1, 1, -1, -1]\n",
        "#heaviside threshold and in the question we got 8 classifier \n",
        "heaviside_function_thresholds = [-0.5, -0.5, 0.5, 0.5, -0.5, -0.5, 0.5, 0.5]\n",
        "#heaviside postive or negative depending on the threshold above which class it belongs too \n",
        "heaviside_function_classes = [+1, -1, +1, -1, +1, -1, +1, -1]\n",
        "main(ip=ip, targets=targets,\n",
        "     heaviside_function_thresholds=heaviside_function_thresholds, heaviside_function_classes=heaviside_function_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "      0    1    2    3     4\n",
            "C1  1.0 -1.0  1.0  1.0  0.75\n",
            "C2 -1.0  1.0 -1.0 -1.0  0.25\n",
            "C3  1.0 -1.0 -1.0 -1.0  0.25\n",
            "C4 -1.0  1.0  1.0  1.0  0.75\n",
            "C5  1.0 -1.0  1.0  1.0  0.75\n",
            "C6 -1.0  1.0 -1.0 -1.0  0.25\n",
            "C7  1.0 -1.0 -1.0 -1.0  0.25\n",
            "C8 -1.0  1.0  1.0  1.0  0.75\n",
            "WEIGHTS ARE: \n",
            "[0.5        0.16666667 0.16666667 0.16666667]\n",
            " alpha part Classified for this iteration is (log applying) 0.5493061443340549*C2\n",
            "Next Iteration(y/n)y\n",
            "\n",
            "      0    1    2    3         4\n",
            "C1  1.0 -1.0  1.0  1.0  0.500000\n",
            "C2 -1.0  1.0 -1.0 -1.0  0.500000\n",
            "C3  1.0 -1.0 -1.0 -1.0  0.166667\n",
            "C4 -1.0  1.0  1.0  1.0  0.833333\n",
            "C5  1.0 -1.0  1.0  1.0  0.500000\n",
            "C6 -1.0  1.0 -1.0 -1.0  0.500000\n",
            "C7  1.0 -1.0 -1.0 -1.0  0.166667\n",
            "C8 -1.0  1.0  1.0  1.0  0.833333\n",
            "WEIGHTS ARE: \n",
            "[0.3 0.5 0.1 0.1]\n",
            " alpha part Classified for this iteration is (log applying) 0.8047189562170503*C3\n",
            "Next Iteration(y/n)y\n",
            "\n",
            "      0    1    2    3    4\n",
            "C1  1.0 -1.0  1.0  1.0  0.7\n",
            "C2 -1.0  1.0 -1.0 -1.0  0.3\n",
            "C3  1.0 -1.0 -1.0 -1.0  0.5\n",
            "C4 -1.0  1.0  1.0  1.0  0.5\n",
            "C5  1.0 -1.0  1.0  1.0  0.7\n",
            "C6 -1.0  1.0 -1.0 -1.0  0.3\n",
            "C7  1.0 -1.0 -1.0 -1.0  0.5\n",
            "C8 -1.0  1.0  1.0  1.0  0.5\n",
            "WEIGHTS ARE: \n",
            "[0.42 0.3  0.14 0.14]\n",
            " alpha part Classified for this iteration is (log applying) 0.4236489301936017*C2\n",
            "Next Iteration(y/n)n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ZA1PMVyih_"
      },
      "source": [
        "#Week 10\n",
        "\n",
        "K means\n",
        "\n",
        "#change to manhattan if it;s asked in the exam\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlR9-QihpnPg",
        "outputId": "35b71688-f94e-45bf-bcfd-a5251bfd4e49"
      },
      "source": [
        "#change to manhattan if it;s asked in the exam\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Initialisation - Parameters from question\n",
        "\n",
        "k = 2 # Number of clusters to form\n",
        "\n",
        "centers = {  # Initial clusters' centers\n",
        "    0: [-1, 3],\n",
        "    1: [5, 1],\n",
        "}\n",
        "\n",
        "x = np.array([  # Dataset\n",
        "    [-1, 3],\n",
        "    [1, 4],\n",
        "    [0, 5],\n",
        "    [4, -1],\n",
        "    [3, 0],\n",
        "    [5, 1]\n",
        "])\n",
        "\n",
        "try:\n",
        "    assert(k == len(centers))\n",
        "except AssertionError as e:\n",
        "    e.args += ('The number of initialised clusers doesn\\'t match k',\n",
        "               len(centers), k)\n",
        "    raise\n",
        "\n",
        "print(\n",
        "    f'The parameters are: k = {k} and the centers are {centers}')\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "#change to manhattan if it's asked in the exam \n",
        "\n",
        "def distance(feature, centers, method='euclidian'):\n",
        "    if method == 'euclidian':\n",
        "        return [np.linalg.norm(feature-centers[center])  # euclidian norm\n",
        "                for center in centers]\n",
        "    if method == 'manhatan':\n",
        "        return [np.linalg.norm(feature-centers[center], 1)  # manhatan dist\n",
        "                for center in centers]\n",
        "\n",
        "\n",
        "previous = {}\n",
        "for i in range(5):  # Max number of iterations (I don't think we'd be expected to perform more than 5 iterations)\n",
        "    print(f'\\n Iteration number {i+1}: \\n')\n",
        "    classes = {}  # Dict to hold a list of data points that are closest to the cluster number\n",
        "    for j in range(k):\n",
        "        classes[j] = []  # Instantiate empty list at every iteration\n",
        "    for feature in x:  # For each data point do:\n",
        "        # Compute the distance to each cluster (options: euclidian or manhatan)\n",
        "        distances = distance(feature, centers, 'euclidian')\n",
        "\n",
        "        classification = distances.index(\n",
        "            min(distances))  # Find the lowest distance\n",
        "        # Assign the datapoint to that cluster\n",
        "        classes[classification].append(feature)\n",
        "\n",
        "    # Print to which cluster each data point belongs to\n",
        "    print(f'The classification is: {classes}')\n",
        "\n",
        "    previous = centers.copy()  # Copy the cluster centers dict\n",
        "    print(f'The previous centers are:{previous}')\n",
        "    for classification in classes:\n",
        "        # Compute the new cluster average by taking the mean of all the data point assigned to that cluster\n",
        "        centers[classification] = np.average(classes[classification], axis=0)\n",
        "    print(f'The new centers are:{centers}')\n",
        "\n",
        "    opti = True\n",
        "    for center in centers:\n",
        "        prev = previous[center]  # Previous cluster\n",
        "        curr = centers[center]  # Update cluster\n",
        "        # termination criteria #TODO: IMPORTANT: This can be changed in the exam question!!\n",
        "        if np.sum(curr-prev) != 0:\n",
        "            opti = False\n",
        "    if opti:\n",
        "        break\n",
        "\n",
        "print('\\n The algorithm converged!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters are: k = 2 and the centers are {0: [-1, 3], 1: [5, 1]}\n",
            "\n",
            " Iteration number 1: \n",
            "\n",
            "The classification is: {0: [array([-1,  3]), array([1, 4]), array([0, 5])], 1: [array([ 4, -1]), array([3, 0]), array([5, 1])]}\n",
            "The previous centers are:{0: [-1, 3], 1: [5, 1]}\n",
            "The new centers are:{0: array([0., 4.]), 1: array([4., 0.])}\n",
            "\n",
            " Iteration number 2: \n",
            "\n",
            "The classification is: {0: [array([-1,  3]), array([1, 4]), array([0, 5])], 1: [array([ 4, -1]), array([3, 0]), array([5, 1])]}\n",
            "The previous centers are:{0: array([0., 4.]), 1: array([4., 0.])}\n",
            "The new centers are:{0: array([0., 4.]), 1: array([4., 0.])}\n",
            "\n",
            " The algorithm converged!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhW-kZmD0tQQ"
      },
      "source": [
        "#K means #ignore during exam\n",
        "\n",
        "S = np.asarray([\n",
        "    [-1,3],\n",
        "    [1,4],\n",
        "    [0,5],\n",
        "    [4,-1],\n",
        "    [3,0],\n",
        "    [5,1]\n",
        "])\n",
        "\n",
        "classes = [0 for _ in S]\n",
        "\n",
        "m_1 = np.asarray([-1,3])\n",
        "\n",
        "m_2 = np.asarray([5,1])\n",
        "\n",
        "M = [m_1,m_2]\n",
        "    \n",
        "for ix, s in enumerate(S):\n",
        "    print(f\"sample {s}\")\n",
        "    dists = [np.linalg.norm(s-m) for m in M]\n",
        "    print(f\"dists to centers {dists}\")\n",
        "    print(f\"closest center {np.argmax(dists)}\")\n",
        "    classes[ix] = np.argmin(dists)\n",
        "    print('')\n",
        "\n",
        "unique_classes = set(classes)\n",
        "\n",
        "M = [np.mean([s for s,c in zip(S,classes) if c == c_i], axis=0) for c_i in unique_classes] \n",
        "    \n",
        "print(f\"classes: {classes}\")     # 0: m_1, 1: m_2\n",
        "\n",
        "print(f\"new centers {M, unique_classes}\") # the classes + the labels they belong to"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwGo8Bo_AvZP"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798d5e5a"
      },
      "source": [
        "from scipy.linalg import svd\n",
        "\n",
        "# PCA\n",
        "\n",
        "# INPUTS: \n",
        "    # x: data\n",
        "    # n_components: number of principal components you want to project the data to\n",
        "    # data_to_project: \n",
        "\n",
        "def _PCA(x, n_components, data_to_project=None):\n",
        "    ip = np.array(x)\n",
        "    ip_mean = np.mean(ip, axis=1)\n",
        "    ip_prime = ip - np.vstack(ip_mean)\n",
        "    C = (ip_prime @ ip_prime.T) / ip.shape[1]\n",
        "    V, D, VT = svd(C)\n",
        "    ans = VT @ ip_prime\n",
        "    print(\"-\"*100)\n",
        "    print(\"READ THE ROWS FROM THE TOP\")\n",
        "    print(ans[:n_components])\n",
        "    print(\"-\"*100)\n",
        "    if data_to_project:\n",
        "        data_to_project = np.array(data_to_project)\n",
        "        print(\"-\"*100)\n",
        "        print(f\"PROJECTION OF {data_to_project}\")\n",
        "        print((VT@data_to_project)[:n_components])\n",
        "        print(\"-\"*100)\n",
        "\n",
        "\n",
        "# REPLACE ACCORDING TO THE QUESTION\n",
        "x = [[4, 0, 2, -2], [2, -2, 4, 0], [2, 2, 2, 2]]\n",
        "#components \n",
        "n_components = 2\n",
        "data_to_project = [3, -2, 5]\n",
        "_PCA(x, n_components, data_to_project)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8De1PQnCiWK"
      },
      "source": [
        "#Competitive Learning\n",
        "Question 3\n",
        "#Find the cluster centres for each iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wjP6A0RCmFq"
      },
      "source": [
        "#Find the cluster centres for each iteration.\n",
        "X = np.asarray([\n",
        "    [-1,3],\n",
        "    [1,4],\n",
        "    [0,5],\n",
        "    [4,-1],\n",
        "    [3,0],\n",
        "    [5,1]\n",
        "])\n",
        "\n",
        "\n",
        "m_1 = X[0]/2\n",
        "m_2 = X[2]/2\n",
        "m_3 = X[4]/2\n",
        "\n",
        "M = [m_1, m_2, m_3]\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "X_ordered = [\n",
        "    X[2],\n",
        "    X[0],\n",
        "    X[0],\n",
        "    X[4],\n",
        "    X[5]\n",
        "]\n",
        "\n",
        "for x in X_ordered:\n",
        "    clust_dists = [np.linalg.norm(x-m) for m in M]\n",
        "    print(f\"cluster distances: {clust_dists}\")\n",
        "    \n",
        "    min_cluster = np.argmin(clust_dists)\n",
        "    print(f\"closest cluster: {min_cluster}\")\n",
        "    \n",
        "    M[min_cluster] += lr*(x - M[min_cluster])\n",
        "    \n",
        "    print(f\"closest cluster updated: {M[min_cluster]}\")\n",
        "    \n",
        "    print('')\n",
        "\n",
        "#cluster distanace is correct\n",
        "#closet cluster 0 is 1 and 1 is 2\n",
        "#updated cluster is correct\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAxucNsWHVr9",
        "outputId": "9e524947-58c5-4695-84d5-c0fb6381d923"
      },
      "source": [
        "# classify new data if question asked after iterations and using that at the end updated cluster center\n",
        "\n",
        "print('distances from centers')\n",
        "[print(x, np.around(d, 4)) for x,d in zip(X, [[np.linalg.norm(x-m) for m in M] for x in X])]\n",
        "print('')\n",
        "\n",
        "print('cluster assignments: ')\n",
        "print([np.argmin([np.linalg.norm(x-m) for m in M]) for x in X])\n",
        "\n",
        "\n",
        "# classify new data \n",
        "\n",
        "\n",
        "\n",
        "x_new = [0,-2]\n",
        "\n",
        "\n",
        "print('')\n",
        "print(f\"new sample dists {[round(np.linalg.norm(x_new-m),4) for m in M]}\")\n",
        "print('new sample assignment: ')\n",
        "print(np.argmin([np.linalg.norm(x_new-m) for m in M]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distances from centers\n",
            "[-1  3] [0.559  5.5902]\n",
            "[1 4] [1.82   4.6098]\n",
            "[0 5] [1.6771 6.0208]\n",
            "[ 4 -1] [6.5431 1.5   ]\n",
            "[3 0] [5.1296 1.118 ]\n",
            "[5 1] [6.27  1.118]\n",
            "\n",
            "cluster assignments: \n",
            "[0, 0, 0, 1, 1, 1]\n",
            "\n",
            "new sample dists [5.5509, 4.717]\n",
            "new sample assignment: \n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VizJog3wFles"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "basic leader follower algo\n",
        "\n",
        "no normalisation\n",
        "\n",
        "same dataset as before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r-xnyooQZvU"
      },
      "source": [
        "X = np.asarray([\n",
        "    [-1,3],  #0\n",
        "    [1,4],  #1\n",
        "    [0,5],  #2\n",
        "    [4,-1],  #3\n",
        "    [3,0],  #4\n",
        "    [5,1] #5\n",
        "])\n",
        "\n",
        "theta = 3\n",
        "lr = 0.5\n",
        "\n",
        "#this part is based on the ordered that asked in the question\n",
        "#set the ordered based in the question asked\n",
        "X_ordered = np.asarray([\n",
        "    X[2], \n",
        "    X[0],\n",
        "    X[0],\n",
        "    X[4],\n",
        "    X[5]\n",
        "])\n",
        "\n",
        "\n",
        "M = np.asarray([[]])\n",
        "\n",
        "for x in X_ordered:\n",
        "\n",
        "    if M.size == 0:\n",
        "        M = np.array(x, copy=True)\n",
        "        M = M.reshape(1,2)\n",
        "    \n",
        "    print(f\"cluster centers: {M}\")\n",
        "    \n",
        "    clust_dists = [np.linalg.norm(x-m) for m in M]\n",
        "    print(f\"cluster distances: {clust_dists}\")\n",
        "    \n",
        "    min_cluster = np.argmin(clust_dists)\n",
        "    print(f\"closest cluster: {min_cluster}\")\n",
        "    \n",
        "    if clust_dists[min_cluster] < theta:\n",
        "        \n",
        "        delta_m = lr*(x - M[min_cluster])\n",
        "        \n",
        "        M = list(M)\n",
        "        M[min_cluster] = M[min_cluster] + delta_m\n",
        "        M = np.array(M)\n",
        "        \n",
        "        print(f\"closest cluster updated: {M[min_cluster]}\")\n",
        "    else:\n",
        "        print('no update, ie min dist < theta')\n",
        "        M = np.array([M,x])\n",
        "        \n",
        "    print('')\n",
        "    \n",
        "print(f\"final clusters {M}\")    \n",
        "#closet cluster 0 is 1 and 1 is 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nj38tKwHKl6",
        "outputId": "1c21d154-e95c-491f-f7e1-c85b8e614224"
      },
      "source": [
        "\n",
        "#classify for new data points if question asked \n",
        "\n",
        "print('distances from centers')\n",
        "[print(x, np.around(d, 4)) for x,d in zip(X, [[np.linalg.norm(x-m) for m in M] for x in X])]\n",
        "print('')\n",
        "\n",
        "print('cluster assignments: ')\n",
        "print([np.argmin([np.linalg.norm(x-m) for m in M]) for x in X])\n",
        "\n",
        "\n",
        "\n",
        "#classify for new data points if question asked \n",
        "x_new = [0,-2]\n",
        "\n",
        "print('')\n",
        "print(f\"new sample dists {[round(np.linalg.norm(x_new-m),4) for m in M]}\")\n",
        "print('new sample assignment: ')\n",
        "print(np.argmin([np.linalg.norm(x_new-m) for m in M]))\n",
        "\n",
        "#closet cluster 0 is 1 and 1 is 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distances from centers\n",
            "[-1  3] [0.559  5.5902]\n",
            "[1 4] [1.82   4.6098]\n",
            "[0 5] [1.6771 6.0208]\n",
            "[ 4 -1] [6.5431 1.5   ]\n",
            "[3 0] [5.1296 1.118 ]\n",
            "[5 1] [6.27  1.118]\n",
            "\n",
            "cluster assignments: \n",
            "[0, 0, 0, 1, 1, 1]\n",
            "\n",
            "new sample dists [5.5509, 4.717]\n",
            "new sample assignment: \n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzreXzduIsTU"
      },
      "source": [
        "#Fuzzy K means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBPnAY0NIrG2"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True) \n",
        "\n",
        "\n",
        "\n",
        "def fuzzy_k_means(data, K, b, iterations, weights):\n",
        "    data = np.array(data)\n",
        "    weights = np.array(weights)\n",
        "    centers = [None] * K\n",
        "    # centers = np.array(centers)\n",
        "    data = np.array(data)\n",
        "    for _iter in range(iterations):\n",
        "        # calculate centers\n",
        "        for idx, w in enumerate(weights):\n",
        "            new_center = [None] * data.shape[1]\n",
        "            for idy in range(data.shape[1]):\n",
        "                new_center[idy] = w[idy]**b * data[:, idy]\n",
        "            centers[idx] = (np.array(new_center).sum(axis=0)/(w**b).sum())\n",
        "        # print(\"Centers\", centers)\n",
        "        new_weight_container = [None] * data.shape[1]\n",
        "        for idx in range(data.shape[1]):\n",
        "            new_weights = []\n",
        "            for c in centers:\n",
        "                val = np.linalg.norm(c - data[:, idx])\n",
        "\n",
        "                val = (1 / val) ** (2/(b-1))\n",
        "                new_weights.append(val)\n",
        "            new_weights = np.array(new_weights)\n",
        "            nw_sum = new_weights.sum()\n",
        "            for idy, nw in enumerate(new_weights):\n",
        "                new_weights[idy] = nw/nw_sum\n",
        "            new_weight_container[idx] = new_weights\n",
        "\n",
        "        new_weight_container = np.array(new_weight_container)\n",
        "        for col_idx in range(new_weight_container.shape[1]):\n",
        "            weights[col_idx] = new_weight_container[:, col_idx]\n",
        "        print(F\"CENTERS AFTER ITERATION {_iter+1} [READ COLUMN WISE]\")\n",
        "        print(np.array(centers).T)\n",
        "        print(\"#\"*100)\n",
        "        print(f\"WEIGHTS AFTER ITERATION {_iter + 1} [READ COLUMN WISE]\")\n",
        "        print(weights.T)\n",
        "        print(\"-\"*100)\n",
        "\n",
        "\n",
        "# REPLACE A VALUES AS GIVEN IN THE QUESTION it's S\n",
        "data = [[-1, 1, 0, 4, 3, 5], [3, 4, 5, -1, 0, 1]]\n",
        "# REPLACE A VALUES AS GIVEN IN THE QUESTION\n",
        "#K is the value given in the question  and weight is miu/membership\n",
        "fuzzy_k_means(data=data, K=2, weights=[\n",
        "              [1, 0.5, 0.5, 0.5, 0.5, 0], [0, 0.5, 0.5, 0.5, 0.5, 1]], b=2, iterations=3)\n",
        "#weights after iteration are the membership u1j and u2j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MOhLZVvs6_y"
      },
      "source": [
        "#Extra K means FUzzy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I873WHhSs373"
      },
      "source": [
        "#extra K means fuzzy\n",
        "\n",
        "\n",
        "X = np.asarray([\n",
        "    [-1,3],\n",
        "    [1,4],\n",
        "    [0,5],\n",
        "    [4,-1],\n",
        "    [3,0],\n",
        "    [5,1]\n",
        "])\n",
        "\n",
        "K = 2\n",
        "b = 2\n",
        "threshhold = 0.5\n",
        "\n",
        "mu = [\n",
        "    [1,0],\n",
        "    [0.5,0.5],\n",
        "    [0.5,0.5],\n",
        "    [0.5,0.5],\n",
        "    [0.5,0.5],\n",
        "    [0,1]\n",
        "]\n",
        "\n",
        "# cluster centers m_i\n",
        "M = [0 for _ in range(K)]\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "    \n",
        "    print(f\"iter: {_}\", '\\n')\n",
        "    \n",
        "    # normalize memberships\n",
        "    mu = [i / np.sum(i) for i in mu]\n",
        "    \n",
        "    print(f\"normalized memberships\")\n",
        "    print(np.around(mu, 4), '\\n')\n",
        "    \n",
        "    # update cluster centers\n",
        "    \n",
        "    M_old = np.array(M)\n",
        "    \n",
        "    for i, m_i in enumerate(M):\n",
        "        \n",
        "        sum_square_mu_x = [(j[i]**b)*np.asarray(x) for j,x in zip(mu,X)]\n",
        "        sum_square_mu_x = np.sum(sum_square_mu_x, axis=0)\n",
        "        \n",
        "        sum_square_mu = (sum(j[i]**b for j in mu))\n",
        "        \n",
        "        M[i] = sum_square_mu_x/sum_square_mu\n",
        "        \n",
        "    print(f\"updated cluster centers:\")\n",
        "    print(np.around(M,4), '\\n')\n",
        "    \n",
        "    change_in_M = np.asarray(M_old) - np.asarray(M)\n",
        "    \n",
        "    # check for convergeance\n",
        "    print(\"difference of old and new clusters\")\n",
        "    print(change_in_M, '\\n')\n",
        "    \n",
        "    if (abs(change_in_M) < 0.5).all():\n",
        "        print('converged')\n",
        "        break\n",
        "        \n",
        "    # update memberships \n",
        "    for i, x in enumerate(X):\n",
        "        \n",
        "        cluster_dists = [np.linalg.norm(x-m_i) for m_i in M]\n",
        "        \n",
        "        vars_ = [(1/k)**(2/(b-1)) for k in cluster_dists]\n",
        "        \n",
        "        mu_ij = [(var / (sum(vars_))) for var in vars_]\n",
        "        \n",
        "        mu[i] = [mu_ij[0], mu_ij[1]]\n",
        "        \n",
        "    print(f\"updated memberships (not normalized)\")\n",
        "    print(np.around(mu, 4), '\\n')\n",
        "\n",
        "    if (abs(change_in_M) < 0.5).all():\n",
        "        print('converged')\n",
        "        break\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvdjoEBPtAv1"
      },
      "source": [
        "#Question 6 \n",
        "\n",
        "Agglomerative clustering until three clusters formed (c=3). use Eulicidean distance as the similarity metric. To determine the distance between clusters use single-link clustering( the minimum distance between samples in the two clusters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMZ4WtvatpLg"
      },
      "source": [
        "import numpy as np \n",
        "X = [\n",
        "    [-1,3],\n",
        "    [1,2],\n",
        "    [0,1],\n",
        "    [4,0],\n",
        "    [5,4],\n",
        "    [3,2]\n",
        "]\n",
        "\n",
        "c = 3\n",
        "\n",
        "clusters = X\n",
        "\n",
        "\n",
        "def proxim_matrix(clusters):\n",
        "    return [[np.linalg.norm(np.asarray(x_i) - np.asarray(x_j)) for x_i in X] for x_j in X]\n",
        "\n",
        "proximity_matrix = np.asarray(proxim_matrix(clusters))\n",
        "\n",
        "proximity_matrix[proximity_matrix == 0] = 999\n",
        "\n",
        "print(pd.DataFrame(proximity_matrix))\n",
        "\n",
        "min_ix = 8%len(clusters), 8//len(clusters)\n",
        "\n",
        "proximity_matrix[min_ix] = 999"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}